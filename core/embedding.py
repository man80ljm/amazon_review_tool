import os
import hashlib
import numpy as np
from typing import List, Optional

from sentence_transformers import SentenceTransformer


class Embedder:
    def __init__(self, model_name: str, batch_size: int = 32):
        if not model_name:
            raise ValueError("âŒ Embedder: model_name ä¸èƒ½ä¸ºç©º")
        
        self.model_name = model_name
        
        # ğŸ”¥ é˜²å¾¡ï¼šbatch_size å¯èƒ½æ˜¯ None
        if batch_size is None or batch_size <= 0:
            print(f"âš ï¸ batch_size æ— æ•ˆ ({batch_size})ï¼Œä½¿ç”¨é»˜è®¤å€¼ 32")
            batch_size = 32
        self.batch_size = int(batch_size)
        
        # é˜²å¾¡ï¼šæ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.isdir(model_name):
            print(f"âš ï¸ WARNING: {model_name} ä¸æ˜¯æœ¬åœ°ç›®å½•ï¼Œå°†å°è¯•ä» HuggingFace ä¸‹è½½")
        
        try:
            #âœ… å…³é”®ä¿®å¤ï¼šåŠ è½½æ¨¡å‹æ—¶ä½¿ç”¨ resolve_model_path
            from core.io_utils import resolve_model_path
            real_model_path = resolve_model_path(self.model_name)
            self.model = SentenceTransformer(
                real_model_path,
                device="cpu",  # æ‰“åŒ…åå»ºè®®é” CPU
                model_kwargs={"local_files_only": True}
            )
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½ embedding æ¨¡å‹å¤±è´¥: {model_name}\né”™è¯¯: {e}")

    def _make_cache_key(self, texts: List[str]) -> str:
        """
        æ ¹æ®æ–‡æœ¬å†…å®¹ + æ¨¡å‹å ç”Ÿæˆç¨³å®š hash
        """
        h = hashlib.sha256()
        h.update(self.model_name.encode("utf-8"))

        # åªå–å‰åæ–‡æœ¬ï¼Œé¿å…è¶…å¤§å­—ç¬¦ä¸²
        if texts:
            h.update(str(len(texts)).encode("utf-8"))
            h.update(str(texts[0][:200]).encode("utf-8"))
            h.update(str(texts[-1][:200]).encode("utf-8"))

        return h.hexdigest()[:16]

    def encode(
        self,
        texts,  # æ•…æ„ä¸å†™ç±»å‹ï¼Œå…¼å®¹å„ç§ä¼ å…¥
        cache_path: Optional[str] = None,
        progress=None
    ) -> np.ndarray:
        """
        è®¡ç®—æ–‡æœ¬ embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        ğŸ”¥ ç»ˆæé˜²å¾¡ç‰ˆï¼šæ— è®ºä¼ å…¥ä»€ä¹ˆåƒåœ¾æ•°æ®éƒ½ä¸ä¼šå´©æºƒ
        """
        
        # ============ ç¬¬1å±‚é˜²å¾¡ï¼šç±»å‹æ£€æŸ¥ ============
        if texts is None:
            raise ValueError(
                "âŒ encode(): texts å‚æ•°ä¸º None!\n"
                "è¿™é€šå¸¸è¯´æ˜ df_work['_text'] æœ‰é—®é¢˜ã€‚\n"
                "è¯·æ£€æŸ¥æ•°æ®åŠ è½½å’Œè¿‡æ»¤æ­¥éª¤ã€‚"
            )
        
        # å¦‚æœæ˜¯ Seriesï¼Œè½¬åˆ—è¡¨
        if hasattr(texts, 'tolist'):
            texts = texts.tolist()
        
        # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•è½¬æ¢
        if not isinstance(texts, list):
            try:
                texts = list(texts)
            except Exception as e:
                raise ValueError(
                    f"âŒ encode(): texts æ— æ³•è½¬æ¢ä¸ºåˆ—è¡¨!\n"
                    f"å½“å‰ç±»å‹: {type(texts)}\n"
                    f"é”™è¯¯: {e}"
                )
        
        # ============ ç¬¬2å±‚é˜²å¾¡ï¼šå†…å®¹æ£€æŸ¥ ============
        if len(texts) == 0:
            raise ValueError(
                "âŒ encode(): texts ä¸ºç©ºåˆ—è¡¨!\n"
                "è¿™è¯´æ˜è¿‡æ»¤åæ²¡æœ‰æ•°æ®äº†ã€‚\n"
                "å»ºè®®ï¼šå–æ¶ˆ'ä»…è´Ÿé¢'æˆ–è°ƒæ•´è¿‡æ»¤ç­–ç•¥ã€‚"
            )
        
        # ============ ç¬¬3å±‚é˜²å¾¡ï¼šæ¸…æ´—æ•°æ® ============
        # æŠŠæ‰€æœ‰ None/NaN è½¬æˆç©ºå­—ç¬¦ä¸²
        cleaned_texts = []
        for i, t in enumerate(texts):
            if t is None or (isinstance(t, float) and np.isnan(t)):
                cleaned_texts.append("")
            else:
                cleaned_texts.append(str(t).strip())
        
        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯ç©ºæ–‡æœ¬
        non_empty = sum(1 for t in cleaned_texts if t)
        if non_empty == 0:
            raise ValueError(
                f"âŒ encode(): {len(cleaned_texts)} æ¡æ–‡æœ¬å…¨éƒ¨ä¸ºç©º!\n"
                "è¯·æ£€æŸ¥æ•°æ®ä¸­çš„æ–‡æœ¬åˆ—æ˜¯å¦æœ‰å†…å®¹ã€‚"
            )
        
        print(f"âœ… Embedding æ•°æ®æ£€æŸ¥é€šè¿‡: {len(cleaned_texts)} æ¡, éç©º {non_empty} æ¡")
        
        # ============ ç¼“å­˜é€»è¾‘ ============
        cache_file = None
        if cache_path:
            os.makedirs(cache_path, exist_ok=True)
            key = self._make_cache_key(cleaned_texts)
            cache_file = os.path.join(
                cache_path,
                f"embeddings_{key}.npy"
            )

            if os.path.exists(cache_file):
                try:
                    emb = np.load(cache_file)
                    if emb.shape[0] == len(cleaned_texts):
                        if progress:
                            progress(len(cleaned_texts), len(cleaned_texts), "âœ… Embedding cache loaded")
                        return emb
                    else:
                        print(f"âš ï¸ ç¼“å­˜ç»´åº¦ä¸åŒ¹é…ï¼Œé‡æ–°è®¡ç®—")
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

        # ============ çœŸæ­£è®¡ç®— embedding ============
        if progress:
            progress(0, len(cleaned_texts), "Embedding encoding...")

        try:
            emb = self.model.encode(
                cleaned_texts,
                batch_size=self.batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=False
            )
        except Exception as e:
            raise RuntimeError(
                f"âŒ SentenceTransformer.encode() å¤±è´¥!\n"
                f"æ¨¡å‹: {self.model_name}\n"
                f"æ–‡æœ¬æ•°: {len(cleaned_texts)}\n"
                f"é”™è¯¯: {e}"
            )

        emb = emb.astype(np.float32)

        # ============ ä¿å­˜ç¼“å­˜ ============
        if cache_file:
            try:
                np.save(cache_file, emb)
                print(f"âœ… ç¼“å­˜å·²ä¿å­˜: {cache_file}")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥ï¼ˆä¸å½±å“è¿è¡Œï¼‰: {e}")

        if progress:
            progress(len(cleaned_texts), len(cleaned_texts), "âœ… Embedding done")

        return emb